{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "import time\n",
    "import warnings\n",
    "\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np  # linear algebra\n",
    "import pandas as pd  # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import PIL\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "from IPython.display import clear_output\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from skimage.color import lab2rgb, rgb2lab\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import (Activation, BatchNormalization, Conv2D,\n",
    "                                     Input, InputLayer, LeakyReLU,\n",
    "                                     MaxPooling2D, UpSampling2D)\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import Sequence\n",
    "from tqdm import tqdm\n",
    "\n",
    "##warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loaders & Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "HEIGHT = 128\n",
    "WIDTH  = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataGenerator(Sequence):\n",
    "    def __init__(self, image_paths, batch_size):\n",
    "        self.paths = image_paths\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(len(self.paths) / float(self.batch_size)))\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        images = self.paths[idx * self.batch_size:(idx + 1) * self.batch_size]\n",
    "\n",
    "        \n",
    "        return self.__getImages(images)\n",
    "\n",
    "    def __getImages(self,paths):\n",
    "        X_img=[]\n",
    "        y_img=[]\n",
    "        for imageDir in paths:\n",
    "            try:\n",
    "                image = img_to_array(load_img(imageDir,target_size=(HEIGHT,WIDTH)))\n",
    "                lab_image = rgb2lab( (1/255) * image)\n",
    "                X_img.append(lab_image[:,:,0].reshape(HEIGHT,WIDTH,1))\n",
    "                y_img.append((lab_image[:,:,1:]/128).reshape(HEIGHT,WIDTH,2))\n",
    "            except:\n",
    "                continue\n",
    "        X_img = np.array(X_img)\n",
    "        y_img = np.array(y_img)\n",
    "        return X_img,y_img\n",
    "    def shuffle(self):\n",
    "        np.random.shuffle(self.paths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PARAMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_activation(x):\n",
    "    return ((K.sigmoid(x)-0.5)/0.5)\n",
    "get_custom_objects().update({'custom_activation':Activation(custom_activation)})\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_generator_model():\n",
    "    model = Sequential([\n",
    "    Input(shape=(HEIGHT, WIDTH,1)),\n",
    "    Conv2D(16,(3,3),padding='same',strides=1),\n",
    "    LeakyReLU(),\n",
    "    Conv2D(32,(3,3),padding='same',strides=1),\n",
    "    Conv2D(32,(3,3),padding='same',strides=1),\n",
    "    \n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2),padding='same'),\n",
    "    \n",
    "    Conv2D(64,(3,3),padding='same',strides=1),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    MaxPooling2D(pool_size=(2,2),padding='same'),\n",
    "    \n",
    "    Conv2D(128,(3,3),padding='same',strides=1),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Conv2D(256,(3,3),padding='same',strides=1),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    UpSampling2D((2, 2)),\n",
    "    Conv2D(128,(3,3),padding='same',strides=1),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    UpSampling2D((2, 2)),\n",
    "    Conv2D(64,(3,3), padding='same',strides=1),\n",
    "    LeakyReLU(),\n",
    "    \n",
    "    Conv2D(64,(3,3), padding='same',strides=1),\n",
    "    LeakyReLU(),\n",
    "    BatchNormalization(),\n",
    "    \n",
    "    Conv2D(32,(3,3),padding='same',strides=1),\n",
    "    LeakyReLU(),\n",
    "    \n",
    "    Conv2D(2,(3,3), activation=custom_activation,padding='same',strides=1),\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_discriminator_model():\n",
    "    model = tf.keras.Sequential([\n",
    "        Input(shape=(HEIGHT, WIDTH,2)),\n",
    "        layers.Conv2D(16, (5, 5), strides=(2, 2), padding='same'),\n",
    "        layers.MaxPool2D((2,2)),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Conv2D(32, (3, 3), strides=(2, 2), padding='same'),\n",
    "        layers.MaxPool2D((2,2)),\n",
    "        BatchNormalization(),\n",
    "        layers.LeakyReLU(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Flatten(),\n",
    "        layers.Dense(1, activation='sigmoid')\n",
    "    ])\n",
    "\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def PSNR(y_true,y_pred):\n",
    "\t# print(\"CALLING PSNR\")\n",
    "\treturn tf.image.psnr(y_true,y_pred,2)\n",
    "\n",
    "\n",
    "\n",
    "def discriminator_loss(real_output, fake_output):\n",
    "    real_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(real_output) - tf.random.uniform( shape=real_output.shape , maxval=0.1 ) , real_output)\n",
    "    fake_loss = tf.keras.losses.BinaryCrossentropy()(tf.zeros_like(fake_output) + tf.random.uniform( shape=fake_output.shape , maxval=0.1  ) , fake_output)\n",
    "    total_loss = real_loss + fake_loss\n",
    "    return total_loss\n",
    "\n",
    "def custom_binary(y_true,y_pred):\n",
    "\t# print(\"CALLING CUSTOM BINARY\")\n",
    "\treturn tf.keras.losses.BinaryCrossentropy(label_smoothing=0.1)(tf.convert_to_tensor(np.ones(y_pred.shape),dtype=tf.float32),y_pred)\n",
    "\n",
    "def SSIM(y_true, y_pred):\n",
    "  return tf.reduce_mean(tf.image.ssim(y_true +1 , y_pred+1, 2.0))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gan():\n",
    "    discriminator = make_discriminator_model()\n",
    "    generator = make_generator_model()\n",
    "\n",
    "    return generator,discriminator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gan Created"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator, discriminator = create_gan()\n",
    "\n",
    "\n",
    "generator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)\n",
    "discriminator_optimizer = tf.keras.optimizers.Adam(0.0002, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "imagepaths= os.listdir(\"../test2017\")\n",
    "imagepaths = list(map(lambda x:'../test2017/'+x,imagepaths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input,reals = DataGenerator(imagepaths,4).__getitem__(0)\n",
    "def generate_and_save_images(model,epoch):\n",
    "\n",
    "  predictions = model.predict(test_input)\n",
    "\n",
    "  fig = plt.figure(figsize=(8, 8))\n",
    "  fig.tight_layout()\n",
    "  \n",
    "  with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    for i,(lmap,pred,real) in enumerate(list(zip(test_input,predictions,reals))):\n",
    "        plt.subplot(4, 2, 2*(i+1))\n",
    "        img = np.zeros((WIDTH,HEIGHT,3)).reshape(HEIGHT,WIDTH,3)\n",
    "        \n",
    "        img[:,:,0] =lmap.reshape(HEIGHT,WIDTH)\n",
    "        \n",
    "        \n",
    "        img[:,:,1:] = pred.reshape(HEIGHT,WIDTH,2) *128\n",
    "        # print(\"PREDICTION\",img[:,:,1:].min(),img[:,:,1:].max())\n",
    "        img = img \n",
    "\n",
    "        rgb_img = lab2rgb(img)\n",
    "        plt.imshow(rgb_img)\n",
    "        plt.axis('off')\n",
    "        plt.subplot(4,2,2*(i+1) -1)\n",
    "      \n",
    "        img[:,:,0] =lmap.reshape(HEIGHT,WIDTH) \n",
    "        img[:,:,1:] = real.reshape(HEIGHT,WIDTH,2)*128 \n",
    "        img = img\n",
    "        rgb_img = lab2rgb(img)\n",
    "\n",
    "      \n",
    "\n",
    "\n",
    "        plt.imshow(rgb_img)\n",
    "        plt.axis('off')\n",
    "\n",
    "\n",
    "  plt.savefig('./Results/image_at_epoch_{:06d}.png'.format(epoch))\n",
    "  plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_dir = './training_checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
    "                                 discriminator_optimizer=discriminator_optimizer,\n",
    "                                 generator=generator,\n",
    "                                 discriminator=discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step( input_x , real_y ):\n",
    "   \n",
    "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "        # Generate an image -> G( x )\n",
    "        generated_images = generator( input_x , training=True)\n",
    "        # Probability that the given image is real -> D( x )\n",
    "        real_output = discriminator( real_y, training=True)\n",
    "        # Probability that the given image is the one generated -> D( G( x ) )\n",
    "        generated_output = discriminator(generated_images, training=True)\n",
    "        \n",
    "        # PSNR\n",
    "        PSNR_loss = tf.reduce_mean(PSNR( real_y, generated_images ))\n",
    "\n",
    "        MAE_LOSS = tf.keras.losses.MeanAbsoluteError()(real_y, generated_images)\n",
    "\n",
    "        SSIM_LOSS = tf.reduce_mean(SSIM(real_y, generated_images))  * -1\n",
    "\n",
    "        disc_gen_loss = tf.keras.losses.BinaryCrossentropy()(tf.ones_like(generated_output), generated_output, label_smoothing)\n",
    "        \n",
    "        gen_loss =  PSNR_loss + MAE_LOSS + SSIM_LOSS + disc_gen_loss\n",
    "\n",
    "        # Log loss for the discriminator\n",
    "        disc_loss = discriminator_loss( real_output, generated_output )\n",
    "    \n",
    "    #tf.keras.backend.print_tensor( tf.keras.backend.mean( gen_loss ) )\n",
    "    #tf.keras.backend.print_tensor( gen_loss + disc_loss )\n",
    "\n",
    "    # Compute the gradients\n",
    "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
    "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
    "\n",
    "    # Optimize with Adam\n",
    "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
    "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
    "    return PSNR_loss,MAE_LOSS,SSIM_LOSS,disc_gen_loss,gen_loss ,disc_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs=1000,batch_size=16,steps_epoch=50,save=100,logfile='logs.txt'):\n",
    "  data_gen = DataGenerator(imagepaths,batch_size)\n",
    "  print(data_gen.__len__())\n",
    "  for epoch in range(epochs):\n",
    "    start = time.time()\n",
    "    losses = np.asarray([0,0,0,0,0,0])\n",
    "    \n",
    "\n",
    "    for _ in range(steps_epoch):\n",
    "      images,labels = data_gen.__getitem__(0)\n",
    "      data_gen.shuffle()\n",
    "      loss_temp = train_step(images,labels)\n",
    "\n",
    "      losses =np.add(losses,loss_temp)\n",
    "\n",
    "  \n",
    "    \n",
    "    losses = losses / steps_epoch\n",
    "    # Save the model every 15 epochs\n",
    "    if (epoch ) % save == 0:\n",
    "      generate_and_save_images(generator,epoch)\n",
    "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
    "    \n",
    "    print ('Time for epoch {} is {} sec'.format(epoch + 1, time.time()-start))\n",
    "    print('Epoch {} , PSNR {}, MAE {}, SSIM {} , GDL {}, TL {}, DL {}'.format(epoch+1,losses[0],losses[1],losses[2],losses[3]*100,losses[4],losses[5]*100))\n",
    "    clear_output(wait=True)\n",
    "    file_object = open(logfile, 'a')\n",
    "    file_object.write('Epoch {} , PSNR {}, MAE {}, SSIM {} , GDL {}, TL {}, DL {} \\n'.format(epoch+1,losses[0],losses[1],losses[2],losses[3]*100,losses[4],losses[5]*100))\n",
    "    file_object.close()\n",
    "\n",
    "  # Generate after the final epoch\n",
    "\n",
    "  generate_and_save_images(generator,\n",
    "                           epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for epoch 100 is 40.35902738571167 sec\n",
      "Epoch 100 , PSNR 6.227766633033752, MAE 0.9622446471452712, SSIM 0.2201284047961235 , GDL 280.84966111183167, TL 10.218636274337769, DL 42.010133773088455\n"
     ]
    }
   ],
   "source": [
    "train(100,16,100,20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(imagep):\n",
    "    image = img_to_array(load_img(imagep,target_size=(HEIGHT,WIDTH)))\n",
    "    lab_image = rgb2lab( (1/255) * image)\n",
    "    x = lab_image[:,:,0].reshape(1,HEIGHT,WIDTH,1)\n",
    "   \n",
    "    predictions = generator.predict(x)\n",
    "    img = np.zeros((WIDTH,HEIGHT,3)).reshape(HEIGHT,WIDTH,3)\n",
    "    print(predictions.shape)\n",
    "    img[:,:,0] =x.reshape(HEIGHT,WIDTH)\n",
    "    img[:,:,1:] = predictions.reshape(HEIGHT,WIDTH,2) *128\n",
    "    img = lab2rgb(img)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    # plt.imshow(image/255)\n",
    "    plt.axis('off')\n",
    "    # plt.show()\n",
    "\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_image(r\"E:\\Manmade\\Fotos\\ANIME PICS\\PicsArt_02-28-05.23.57.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ce38e80e5198e1a528032d773ef998b5a6815c06ae2381a5a6ff105db93cfe61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
